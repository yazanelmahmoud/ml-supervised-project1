============================================================
DECISION TREE — RESULTS (Adult Income)
============================================================

--- DATA & METHODOLOGY ---
Target: class (<=50K vs >50K); task: binary classification.
Metrics: F1, Accuracy, PR-AUC — imbalance makes accuracy insufficient;
         F1 and PR-AUC better reflect minority-class performance.
Class distribution (train): 27178 <=50K, 8962 >50K (~3.03:1).
Imbalance: minority class (>50K) under-represented; F1/PR-AUC preferred.
Leakage controls: fnlwgt and education dropped (EDA: redundancy, near-zero correlation).
Single held-out test split; tuning via 5-fold CV on training only.

--- Split criterion and justification ---
Criterion: gini. Gini is faster than entropy and yields similar splits;
entropy slightly prefers balanced splits; for this dataset Gini is chosen for speed.

--- Best hyperparameters (CV on training) ---
ccp_alpha: 0.000100
max_depth: 30
min_samples_leaf: 1
Final depth: 19
Number of leaves: 121

--- Best from each model-complexity curve (other params at standard) ---
  ccp_alpha curve (max_depth=None, min_samples_leaf=1):
    best ccp_alpha=0.000100, CV F1=0.6834
  max_depth curve (ccp_alpha=0, min_samples_leaf=1):
    best max_depth=12, CV F1=0.6672
  min_samples_leaf curve (ccp_alpha=0, max_depth=None):
    best min_samples_leaf=50, CV F1=0.6755

--- Grid-search best (joint tuning, val F1) ---
CV F1: 0.6836
Params: {'ccp_alpha': 0.0001, 'max_depth': 30, 'min_samples_leaf': 1}

--- Test metrics ---
Accuracy:  0.8590
F1:        0.6771
PR-AUC:    0.7904

--- Confusion matrix (0=<=50K, 1=>50K, threshold 0.5) ---
TN=6425  FP=370
FN=904  TP=1336

--- Runtime ---
Fit (sec):    0.2083
Predict (sec): 0.0018
Hardware: Windows 10, CPU: Intel64 Family 6 Model 140 Stepping 1, GenuineIntel

--- Note ---
Revisit hypothesis from hypothesis.txt in DT conclusions.

============================================================