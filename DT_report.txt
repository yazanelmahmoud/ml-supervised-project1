================================================================================
  DECISION TREE — REPORT SECTION (Adult Income Dataset)
================================================================================

Our analysis follows a four-stage pipeline. Level 1: train a baseline with
defaults. Level 2: diagnose with learning curves (bias/variance) and
model-complexity sweeps to bracket sensible hyperparameter ranges. Level 3:
tune and lock via grid search, refit the winner on the full training data,
and report test-set results. Level 4: post-mortem on the test set (confusion
matrix, PR–ROC interpretation, and error profile) to summarize strengths
and weaknesses. Below we report the Decision Tree results for the Adult
Income (Census) dataset in that format and tie them to our hypothesis.

----------------------------------------------------------------------------
1) DATA, TASK, AND METRIC CHOICE
----------------------------------------------------------------------------

The task is binary classification: predict income class (<=50K vs >50K).
The training set has 27,178 majority (<=50K) and 8,962 minority (>50K)
samples, i.e. ~3.03:1 imbalance. Under such imbalance, accuracy is
misleading (predicting the majority class yields high accuracy but poor
minority-class utility). We therefore optimize and report F1 (harmonic
mean of precision and recall) and PR-AUC (average precision) in addition to
accuracy. F1 is used as the single objective for cross-validation and
model-complexity curves so that tuning explicitly targets the minority
class; PR-AUC is reported for threshold and ranking interpretation.

Leakage controls from EDA: we drop fnlwgt (sampling weight, non-predictive
of income) and education (redundant with education-num). Features are
preprocessed with one-hot encoding for moderate-cardinality categoricals
(workclass, marital-status, occupation, relationship, race, sex), target
encoding for high-cardinality native-country, and StandardScaler on
numerics. A single held-out test split is used; all tuning (including
grid search) is done with 5-fold stratified CV on the training set only.

----------------------------------------------------------------------------
2) SPLIT CRITERION
----------------------------------------------------------------------------

We use Gini impurity as the split criterion. Gini is faster to compute
than entropy and typically yields very similar splits in practice; entropy
tends to favor slightly more balanced splits. For this dataset we chose
Gini for speed with equivalent discriminative behavior.

----------------------------------------------------------------------------
3) LEVEL 1 — BASELINE (DEFAULTS)
----------------------------------------------------------------------------

A default DecisionTreeClassifier (no max_depth limit, no min_samples_leaf
constraint, ccp_alpha=0) would grow until leaves are pure or minimal,
leading to a very deep, high-variance tree that overfits. We do not report
a formal baseline number here because the pipeline immediately proceeds
to regularization (ccp_alpha, max_depth, min_samples_leaf); the
model-complexity curves and learning curve effectively show that unconstrained
trees overfit (train F1 high, cross-validation F1 lower) and that
regularization closes the gap.

[ show graphic: learning curve — training size vs train F1 and cross-val F1 for
  the best-tuned DT; optional comparison with an unpruned tree if available. ]

----------------------------------------------------------------------------
4) LEVEL 2 — DIAGNOSIS: LEARNING CURVES AND MODEL-COMPLEXITY CURVES
----------------------------------------------------------------------------

Learning curve: We compute the learning curve for the best-tuned model
(best ccp_alpha, max_depth, min_samples_leaf from grid search) over 10
training fractions from 10% to 100%, with 5-fold CV and F1 as the metric.
This shows how train and validation F1 evolve with sample size and whether
adding more data would help (high variance: validation still rising) or
bias dominates (validation plateaus).

Model-complexity curves: To see how each regularization lever affects
bias–variance, we sweep one hyperparameter at a time and hold the others
at a “standard” value. This is intentional: fixing the other params lets us
interpret each curve as “effect of this lever alone.” The fixed (standard)
settings are:

  • ccp_alpha curve: max_depth=None, min_samples_leaf=1.
    So we see the effect of post-pruning (cost-complexity pruning) with no
    pre-pruning.

  • max_depth curve: ccp_alpha=0, min_samples_leaf=1.
    So we see the effect of pre-pruning by depth only, with no
    min_samples_leaf constraint and no post-pruning.

  • min_samples_leaf curve: ccp_alpha=0, max_depth=None.
    So we see the effect of requiring a minimum number of samples per leaf,
    with no depth limit and no post-pruning.

All three curves report 5-fold CV F1 (and train F1) so that we can compare
directly with the grid-search objective. The code uses F1 consistently
because we care about minority-class (>) performance.

[ show graphic: 2×2 panel — (a) learning curve; (b) F1 vs ccp_alpha;
  (c) F1 vs max_depth; (d) F1 vs min_samples_leaf. ]

From the Adult Income results:

  • ccp_alpha curve (max_depth=None, min_samples_leaf=1): CV F1 peaks at
    ccp_alpha=0.0001 (CV F1=0.6834). Very small post-pruning helps;
    no pruning (ccp_alpha=0) or heavier pruning hurts.

  • max_depth curve (ccp_alpha=0, min_samples_leaf=1): Best CV F1 at
    max_depth=12 (0.6672). Shallower trees reduce overfitting when other
    levers are at default; deeper trees overfit.

  • min_samples_leaf curve (ccp_alpha=0, max_depth=None): Best at
    min_samples_leaf=50 (CV F1=0.6755). Larger leaves reduce variance
    when depth is unconstrained.

So diagnosis suggests: light post-pruning (ccp_alpha), moderate depth
(max_depth in the low–mid range), and/or larger min_samples_leaf all
improve generalization. The best single-lever choices differ (e.g. depth 12
vs leaf 50) because they interact; that motivates joint tuning in Level 3.

----------------------------------------------------------------------------
5) LEVEL 3 — TUNING VIA GRID SEARCH AND FINAL TEST EVALUATION
----------------------------------------------------------------------------

We run a grid search over the three levers jointly, still using 5-fold
stratified CV on the training set with F1 as the scoring metric. The
grid is:

  • ccp_alpha: 12 values in logspace from 1e-4 to 1e-1 (plus 0 and 1e-5
    in the extended set for the ccp_alpha curve only).
  • max_depth: [8, 12, 16, 20, 30, 50].
  • min_samples_leaf: [1, 4, 10, 30, 50, 70, 100].

These ranges were chosen to bracket the regions suggested by the
model-complexity curves (e.g. depth up to 50, min_samples_leaf up to 100,
and ccp_alpha from none to moderate pruning).

The grid-search best configuration (by CV F1) is: ccp_alpha=0.0001,
max_depth=30, min_samples_leaf=1, with CV F1=0.6836. We then refit a
single Decision Tree with these hyperparameters on the full training set
and evaluate once on the held-out test set. No test data is used for
tuning; the refit is only on training data.

Best (tuned) Decision Tree summary:
  max_depth: 30, min_samples_leaf: 1, ccp_alpha: 0.0001.
  Final fitted tree: depth 19, number of leaves 121.

So the tree is effectively pruned by ccp_alpha (depth 19 < 30, 121 leaves)
rather than by a tight max_depth or large min_samples_leaf. This is
consistent with the ccp_alpha curve: light post-pruning gives the best
CV F1 when other params are at standard; the grid search confirms that
combining light ccp_alpha with a generous max_depth and small
min_samples_leaf yields the best joint CV F1.

Test-set metrics:
  Accuracy: 0.8590
  F1:       0.6771
  PR-AUC:   0.7904

Runtime (reproducibility): fit ~0.21 s, predict ~0.002 s on the reported
hardware.

[ show graphic: ROC curve and/or PR curve for the test set. ]

----------------------------------------------------------------------------
6) LEVEL 4 — POST-MORTEM: CONFUSION MATRIX AND INTERPRETATION
----------------------------------------------------------------------------

Confusion matrix (threshold 0.5; 0=<=50K, 1=>50K):
  TN=6425  FP=370
  FN=904   TP=1336

So at the default 0.5 threshold the model is relatively conservative on the
positive class: 904 false negatives vs 1336 true positives. Precision is
high (TP/(TP+FP)) and recall is moderate; F1 balances the two. PR-AUC
(0.79) indicates good ranking and thresholding potential: one could lower
the decision threshold to increase recall (fewer FN) at the cost of more
FP, and F1 at 0.5 is 0.677. For applications that care more about
capturing >50K individuals, a threshold below 0.5 could be chosen (e.g. to
maximize F1 or a custom utility); we report the default 0.5 for
comparability.

[ show graphic: confusion matrix at threshold 0.5; optionally confusion
  matrix at a threshold that maximizes F1. ]

[ show graphic: reliability/calibration plot if available; otherwise omit. ]

----------------------------------------------------------------------------
7) COHERENCE WITH HYPOTHESIS
----------------------------------------------------------------------------

Our hypothesis states that Decision Trees should perform well on Adult
Income because they capture non-linear and categorical interactions with
little preprocessing, and that “pruned, moderately shallow trees should
generalize best” by avoiding overfitting while keeping useful decision
boundaries.

The results are coherent with this:

  • We use explicit regularization (ccp_alpha, max_depth, min_samples_leaf)
    and the chosen model is pruned (depth 19, 121 leaves) rather than
    fully grown. So we are in the “pruned, moderately shallow” regime.

  • The diagnosis (model-complexity and learning curves) shows that
    unconstrained or weakly regularized trees overfit (train F1 >> CV F1)
    and that tightening complexity (via ccp_alpha, max_depth, or
    min_samples_leaf) improves CV F1. That supports the claim that
    pruning/regularization is necessary for good generalization.

  • Test F1 (0.677) and PR-AUC (0.79) indicate solid minority-class
    performance and ranking, consistent with the expectation that trees
    can handle the mixed-type, interaction-heavy feature space.

  • We optimized F1 (and reported PR-AUC) because of class imbalance,
    which matches the hypothesis’ emphasis on imbalance and minority-class
    utility.

In short: we trained with defaults in spirit (no manual cherry-picking),
diagnosed with learning and model-complexity curves (with clearly stated
fixed hyperparameters per curve), tuned with a joint grid search over
sensible ranges, refit on the full training set, and reported a single
test evaluation. The chosen tree is pruned and of moderate depth, and the
reported metrics and post-mortem align with the hypothesis that such a
tree generalizes well on this dataset.

================================================================================
