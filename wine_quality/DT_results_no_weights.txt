============================================================
DECISION TREE â€” RESULTS (Wine Quality) [no class weight]
============================================================

--- DATA & METHODOLOGY ---
Target: quality (discrete rating/class, 1-8); task: multiclass classification.
Metrics: Macro-F1 and Accuracy (minimum). Macro-F1 treats all classes equally,
         critical for severe class imbalance (ratio 108.29).
Class distribution (train): {1: 16, 2: 122, 3: 948, 4: 1473, 5: 1013, 6: 533, 7: 138, 8: 13}.
Imbalance ratio (majority/minority): 113.31.
Severe imbalance: minority classes (1, 8) represent <0.4% each;
                   majority class (4) represents 34.6%.
Leakage controls: 'class' column dropped (perfect correlation with 'quality').
Single held-out test split; tuning via 5-fold CV on training only.
No class weighting.

--- Split criterion and justification ---
Criterion: gini. Gini is faster than entropy and yields similar splits;
entropy slightly prefers balanced splits; for this dataset Gini is chosen for speed.

--- Best hyperparameters (CV on training) ---
ccp_alpha: 0.000100
max_depth: 10
min_samples_leaf: 1
class_weight: None
Final depth: 10
Number of leaves: 410

--- Best from each model-complexity curve (other params at standard) ---
  ccp_alpha curve (max_depth=None, min_samples_leaf=1):
    best ccp_alpha=0.001233, CV Macro-F1=0.3179
  max_depth curve (ccp_alpha=0, min_samples_leaf=1):
    best max_depth=12, CV Macro-F1=0.3377
  min_samples_leaf curve (ccp_alpha=0, max_depth=None):
    best min_samples_leaf=20, CV Macro-F1=0.3156

--- Grid-search best (joint tuning, val Macro-F1) ---
CV Macro-F1: 0.3273
Params: {'ccp_alpha': 9.999999999999999e-05, 'max_depth': 10, 'min_samples_leaf': 1}

--- Test metrics ---
Accuracy:     0.4821
Macro-F1:     0.3182
Weighted-F1:  0.4767

--- Per-class performance (F1 scores and Accuracy) ---
  Quality 1: F1 = 0.0000, Accuracy = 0.0000
  Quality 2: F1 = 0.1364, Accuracy = 0.0968
  Quality 3: F1 = 0.5000, Accuracy = 0.4937
  Quality 4: F1 = 0.5338, Accuracy = 0.5679
  Quality 5: F1 = 0.4481, Accuracy = 0.4348
  Quality 6: F1 = 0.4354, Accuracy = 0.4436
  Quality 7: F1 = 0.4918, Accuracy = 0.4412
  Quality 8: F1 = 0.0000, Accuracy = 0.0000

--- Confusion matrix (rows=true, cols=predicted) ---
    Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
Q1    0    0    0    4    0    0    0    0
Q2    0    3   20    8    0    0    0    0
Q3    0    6  117   98   15    1    0    0
Q4    0    3   81  209   68    6    1    0
Q5    0    0   11   78  110   53    1    0
Q6    0    1    2   17   43   59    9    2
Q7    0    0    0    1    2   16   15    0
Q8    0    0    0    0    0    3    1    0

--- Runtime ---
Fit (sec):    0.0214
Predict (sec): 0.0003
Hardware: Linux 6.14.0-1018-aws, CPU: x86_64

--- Note ---
Revisit hypothesis from hypothesis.txt in DT conclusions.

============================================================