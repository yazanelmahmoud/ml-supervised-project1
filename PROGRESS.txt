================================================================================
  PROJECT 1 — ADULT INCOME (CENSUS) — PROGRESS
================================================================================

TASK: Binary classification (<=50K vs >50K), ~3:1 imbalance.
      F1, Accuracy, PR-AUC. EDA → Hypotheses → 4 algos → Interpretation.
      Algos: DT, kNN, SVM, NN (sklearn + PyTorch, SGD only).

--------------------------------------------------------------------------------
COMPLETED
--------------------------------------------------------------------------------
[1] Setup: config, data_loading, preprocessing, eda, models_*, evaluation, utils, notebook.
[2] EDA: class_distribution, missing/dtypes, numeric/categorical summaries, plots → outputs/
[3] EDA Summary (EDA_SUMMARY.txt) + preprocessing.py: drop education/fnlwgt, treat ?, 
    one-hot encode, target-encode native-country, StandardScaler, stratified split.
[4] Hypotheses: hypothesis.txt — theory-based, testable hypotheses for ML experiments.
[5] Preprocessing: implemented in preprocessing.py — drop education/fnlwgt, treat ?, 
    one-hot encode, target-encode native-country, StandardScaler, stratified split.
[6] Decision Trees (DT): split criterion, pruning/regularization, tuning via CV, 
    learning curves, model-complexity curve, runtime table, confusion matrix, 
    final depth/leaves; hypothesis revisited in conclusions.
[7] k-Nearest Neighbors (kNN): model-complexity (k × weights × metric), learning 
    curves, test evaluation, runtime table, confusion matrix; hypothesis revisited.

--------------------------------------------------------------------------------
TODO
--------------------------------------------------------------------------------
• Data/targets/leakage declaration (once, applies to all algos)
• Cross-model comparison & conclusion (after all algos done)

--------------------------------------------------------------------------------
DECISION TREE (DT) — DETAILED STEPS & OUTPUTS (DONE)
--------------------------------------------------------------------------------

STEPS TO DO:
1. State split criterion (Gini or entropy) and why.
2. Use pruning/regularization: post-pruning via ccp_alpha, depth limits, and/or leaf 
   constraints (min_samples_leaf, max_depth). Implement and tune these.
3. Tune hyperparameters via cross-validation on training data only (no test set).
4. Train final model on full training set with chosen hyperparameters.
5. Evaluate on held-out test set once at the end.

FIGURES/TABLES TO GENERATE:
• Learning curves (LC): training and validation metric (F1/Accuracy/PR-AUC) vs. 
  training size (e.g., 10%, 25%, 50%, 75%, 100%). Diagnose bias/variance.
• Model-complexity / pruning curve (MC): validation metric vs. hyperparameter 
  (ccp_alpha or max_depth). Show the regularization/pruning trade-off.
• Runtime table: fit and predict wall-clock times with brief hardware note.
• Confusion matrix at justified operating point (0.5 threshold or other).
• Report: final depth and number of leaves of the chosen model.

DATA & METHODOLOGY (per algo, include for DT):
• Target: class (<=50K vs >50K); task: binary classification.
• Justify metrics: F1, Accuracy, PR-AUC (imbalance makes accuracy insufficient).
• Class distribution (~3:1) and how imbalance affects evaluation.
• Leakage controls: state what was removed/verified (fnlwgt, education, etc.).
• Single held-out test split; tuning via CV on training only.
• Revisit hypothesis from hypothesis.txt in DT conclusions.

--------------------------------------------------------------------------------
k-NEAREST NEIGHBORS (kNN) — STEP-BY-STEP WORKFLOW (DONE)
--------------------------------------------------------------------------------
(Do one step → run code → check results → then say "do step N" or "add step X".)

Step 1 — Model-complexity (k × weights × metric) [merged: no separate “k only” step]
  • Same preprocessed data (StandardScaler). k range e.g. [3, 5, 10, …, 50].
  • For all 4 combos (weights ∈ {uniform, distance}, metric ∈ {euclidean, manhattan}):
    plot CV F1 vs k. Two panels: uniform (euclidean + manhattan) | distance (euclidean + manhattan).
  • Print table at a reference k (e.g. 20) and best (k, weights, metric). Save to KNN_results.txt.

Step 2 — Learning curves
  • Train and validation metric (F1) vs training size (e.g. 10%, 25%, …, 100%).
  • Use (a) a baseline k (e.g. small) and (b) best k from Step 1 (best config).
  • Plot both (e.g. two panels). Append results to KNN_results.txt.

Step 3 — Test evaluation and figures
  • Best config from Step 1 (no separate grid search: same k, weights, metric).
  • Refit best kNN on full training set; evaluate once on held-out test set.
  • Report F1, Accuracy, PR-AUC; fit and predict time (runtime table); hardware note.
  • Confusion matrix at 0.5 (or justified threshold). Append to KNN_results.txt.

Step 4 — Report and interpretation
  • In report: target, task, metrics, class distribution, leakage controls, single 
    test split; best (k, weights, metric) from Step 1.
  • Justify k, distance metric, and weighting. Discuss sensitivity to irrelevant 
    features and high dimensionality. Revisit hypothesis in kNN conclusions.

--------------------------------------------------------------------------------
SUPPORT VECTOR MACHINES (SVM) — STEP-BY-STEP WORKFLOW
--------------------------------------------------------------------------------
(Do one step → run code → check results → then say "do step N" or "add step X".)

Step 1 — Kernel & hyperparameter tuning (linear vs RBF; C, gamma)
  • Same preprocessed data (StandardScaler). Use ≥2 kernels: linear and RBF.
  • Tune C (and gamma for RBF) via CV on training only. Model-complexity curves: 
    plot CV metric vs C (linear) and vs C/gamma (RBF). Two panels or overlaid. 
    Identify best kernel and hyperparams. Save plots to outputs/.

Step 2 — Learning curves
  • Plot train and validation metric (F1) vs training size (e.g. 10%, 25%, …, 100%).
  • Use (a) linear and (b) RBF with chosen hyperparams (or baseline vs best). Two panels.
  • Save to outputs/; append results to SVM_results.txt.

Step 3 — Test evaluation and figures
  • Refit best SVM on full training set; evaluate once on held-out test set.
  • Report F1, Accuracy, PR-AUC; fit and predict time (runtime table); hardware note.
  • Confusion matrix (plot/heatmap) at 0.5 (or justified threshold). Save to outputs/; append to SVM_results.txt.

Step 4 — Report and interpretation
  • Target, task, metrics, class distribution, leakage controls; best kernel and (C, gamma).
  • Discuss scaling, margin, kernel choice. Revisit hypothesis in SVM conclusions.

--------------------------------------------------------------------------------
OTHER ALGOS (after SVM)
--------------------------------------------------------------------------------
• NN sklearn + PyTorch: train, tune, curves, compare

================================================================================
