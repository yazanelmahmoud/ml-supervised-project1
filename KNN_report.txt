================================================================================
  k-NEAREST NEIGHBORS — REPORT SECTION (Adult Income Dataset)
================================================================================

Our analysis follows the same four-stage pipeline as for Decision Trees. Below
we report the kNN results for the Adult Income (Census) dataset, tying them
to model-complexity trade-offs, learning curves, and our hypothesis.

----------------------------------------------------------------------------
1) DATA, TASK, AND METRIC CHOICE
----------------------------------------------------------------------------

The task is binary classification: predict income class (<=50K vs >50K). The
training set has ~3:1 class imbalance (majority <=50K), so we optimize and
report F1 and PR-AUC in addition to accuracy. F1 is used as the single
objective for cross-validation and model-complexity curves. Leakage controls
and preprocessing (one-hot encoding for moderate-cardinality categoricals,
target encoding for native-country, StandardScaler on numerics) match the DT
pipeline; all tuning is done with 5-fold stratified CV on the training set
only, and a single held-out test set is used for final evaluation.

----------------------------------------------------------------------------
2) MODEL-COMPLEXITY CURVES: k, WEIGHTS, AND METRIC
----------------------------------------------------------------------------

We sweep k ∈ {3, 5, 10, 15, 20, 25, 30, 40, 50} for each combination of
weights ∈ {uniform, distance} and metric ∈ {euclidean (L2), manhattan (L1)}.
The curves show classic kNN trade-offs: small k cherry-picks neighbors (low
bias, high variance); medium k smooths by averaging and boosts validation
F1; beyond k≈20–25, performance plateaus with little gain while train score
falls (variance down, bias up).

For Adult Income, the results differ from datasets with heavy one-hot
features (e.g. Hotels). Here, uniform weights with Euclidean (L2) metric
outperform distance weighting and Manhattan (L1):

  • weights=uniform, metric=euclidean: CV F1 peaks at k=25 (0.6640), with
    strong performance at k=15 (0.6636) and k=20 (0.6543).
  • weights=uniform, metric=manhattan: slightly lower, best at k=15 (0.6594)
    and k=25 (0.6592).
  • weights=distance, metric=euclidean: modest generalization bump over
    small k but never surpasses uniform+euclidean (max ~0.6505 at k=50).
  • weights=distance, metric=manhattan: lowest of the four (max ~0.6443).

Practical sweet spot: k ∈ [15, 25] with uniform weights and Euclidean metric.
Distance weighting and L1 do not improve generalization on this dataset,
likely because the encoded feature space (one-hot + numerics) and scaling
lead to different distance behavior than in high-cardinality categorical
spaces where L1 and distance weighting help mitigate noisy neighbors.

[ show graphic: model-complexity curve — CV F1 vs k for each (weights, metric). ]

----------------------------------------------------------------------------
3) LEARNING CURVES
----------------------------------------------------------------------------

Learning curves are computed over 10 training fractions (10% to 100% of
training data) with 5-fold CV. We compare:

  • Baseline: k=5, weights=uniform, metric=euclidean.
  • Tuned: best config from model-complexity sweep — k=25, weights=uniform,
    metric=euclidean.

The curves show classic low-bias, high-variance kNN behavior:

  • Baseline (k=5): train F1 high (~0.74–0.76), validation F1 lower
    (~0.63–0.65). Small k overfits to idiosyncratic micro-clusters; adding
    more data improves val F1 only slowly (0.6344 → 0.6472).
  • Tuned (k=25): train F1 lower (~0.68), validation F1 rises more steadily
    (0.6383 → 0.664). Larger k smooths votes, reduces variance, and yields
    tighter train–val gap. Val F1 improves by ~0.026 from smallest to
    largest training fraction.

So tuning (medium k, uniform neighborhoods) gives a clear validation lift
relative to baseline: val F1 ~0.647 → ~0.664. The reduction in the
train–val gap is modest compared with Decision Trees, where pruning
provides a stronger regularization effect. kNN receives limited lift with
more data because, in a high-dimensional encoded space, many points become
equidistant (distance concentration), and kNN's notion of "similarity"
based on raw feature distances may not align well with meaningful income
predictors—especially for one-hot categorical features where numeric
distance is not semantically meaningful.

[ show graphic: learning curves — train size vs train F1 and val F1 for
  baseline (k=5) and tuned (k=25). ]

----------------------------------------------------------------------------
4) TUNING AND FINAL TEST EVALUATION
----------------------------------------------------------------------------

We select the best configuration from the model-complexity sweep (highest
CV F1) and refit on the full training set. No test data is used for tuning.

Best (tuned) kNN:
  k: 25, weights: uniform, metric: euclidean
  best_cv_f1: 0.6640

Test-set metrics:
  Accuracy: 0.8444
  F1:       0.6594
  PR-AUC:   0.7420

Runtime (reproducibility): fit ~0.002 s, predict ~1.67 s on the reported
hardware. kNN's predict time dominates because it requires computing
distances to all training points for each test instance (no model
compression).

[ show graphic: ROC curve and/or PR curve for the test set. ]

----------------------------------------------------------------------------
5) POST-MORTEM: CONFUSION MATRIX AND INTERPRETATION
----------------------------------------------------------------------------

Confusion matrix (threshold 0.5; 0=<=50K, 1=>50K):
  TN=6268  FP=527
  FN=879   TP=1361

The model is conservative on the positive class: 879 false negatives vs
1361 true positives. Precision is moderate (TP/(TP+FP) ≈ 0.72), recall
moderate (TP/(TP+FN) ≈ 0.61); F1 balances the two at 0.659. PR-AUC (0.742)
indicates reasonable ranking and thresholding potential; one could lower
the decision threshold to increase recall at the cost of more FP. We report
the default 0.5 for comparability with the Decision Tree.

Compared to the tuned Decision Tree (F1 0.677, PR-AUC 0.79), kNN achieves
lower F1 and PR-AUC. Trees can split cleanly on informative categorical
features; kNN's distances in the high-dimensional encoded space make many
points seem similarly far, diluting the true signals and hurting ranking.
This aligns with our hypothesis that kNN would struggle on this dataset.

[ show graphic: confusion matrix at threshold 0.5; optionally calibration
  plot if available. ]

----------------------------------------------------------------------------
6) COHERENCE WITH HYPOTHESIS
----------------------------------------------------------------------------

Our hypothesis states that kNN is expected to perform poorly on Adult Income
because the dataset becomes high-dimensional after one-hot encoding,
leading to distance concentration effects where all points become
equidistant, and because kNN struggles with mixed-type data and categorical
features that lack meaningful distance metrics.

The results are coherent with this:

  • kNN achieves test F1 0.659 and PR-AUC 0.742, vs the Decision Tree's
    F1 0.677 and PR-AUC 0.79. kNN is indeed worse than the tuned tree,
    consistent with the expected ranking (Decision Trees ≈ RBF SVMs ≈ NNs
    > Linear SVMs > kNN).

  • Model-complexity curves show that even with tuning (k=25, uniform,
    euclidean), validation F1 plateaus around 0.66. Learning curves show
    only moderate improvement with more data—suggesting that the curse of
    dimensionality and distance concentration limit kNN's gains.

  • Uniform weights and Euclidean metric work best here, unlike datasets
    (e.g. Hotels) where distance weighting and L1 help. This may reflect
    the specific geometry of the Adult Income encoded space and the mix of
    numeric vs one-hot features.

  • The predict time (~1.67 s) is much larger than the tree (~0.002 s),
    reflecting kNN's non-parametric, instance-based nature—every prediction
    requires scanning the full training set.

In short: kNN is tunable (medium k, uniform, euclidean) and achieves
reasonable F1 and PR-AUC, but it underperforms the Decision Tree and
exhibits the limitations predicted by our hypothesis: difficulty in
high-dimensional encoded spaces and with mixed-type, categorical-heavy
features.

================================================================================
